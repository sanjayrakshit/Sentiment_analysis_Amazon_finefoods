{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import string\n",
    "import scipy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./Reviews.csv').dropna()\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text', 'Summary', 'Score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Score'] != 3]\n",
    "data['Score'] = [1 if item>3 else 0 for item in data['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    for punc in string.punctuation:\n",
    "        if punc != \"\\'\":\n",
    "            x = x.replace(punc, f' {punc} ')\n",
    "    return ' '.join(x.split()).lower()\n",
    "\n",
    "data['Summary'] = [preprocess(item) for item in data['Summary']]\n",
    "data['Text'] = [preprocess(item) for item in data['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [i+' '+j for i,j in zip(list(data['Summary'].values), list(data['Text']))]\n",
    "Y_data = list(data['Score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 133571\n"
     ]
    }
   ],
   "source": [
    "corpus = dict(Counter(' '.join(X_data).split()))\n",
    "print('Number of unique tokens:',len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum word count: 41.0\n"
     ]
    }
   ],
   "source": [
    "min_word_count = np.percentile(list(corpus.values()), 90)\n",
    "print('minimum word count:',min_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 13472\n"
     ]
    }
   ],
   "source": [
    "# Deleting those words which occur less than 41 times\n",
    "words = list(corpus.keys())\n",
    "for w in words:\n",
    "    if corpus[w]<41:\n",
    "        del corpus[w]\n",
    "    \n",
    "print('Number of unique tokens:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable sequence lenght: 199.0\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(item.split()) for item in X_data ]\n",
    "\n",
    "suitable_seq_len = np.percentile(seq_lens, 90)\n",
    "print('Suitable sequence lenght:', suitable_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the interger ids for the words \n",
    "word_ids = {\n",
    "    item: index+1 for index, item in enumerate(corpus.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_int = []; Y_data_new = []\n",
    "for item, y in zip(X_data, Y_data):\n",
    "    temp = [word_ids.get(word, -1) for word in item.split()]\n",
    "    if temp:\n",
    "        X_data_int.append(temp)\n",
    "        Y_data_new.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_int = sequence.pad_sequences(X_data_int, maxlen=int(suitable_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_maker(x):\n",
    "    with tf.Session() as sess:\n",
    "        return sess.run(tf.one_hot(x, depth=len(np.unique(x))))\n",
    "    \n",
    "Y_data_new = one_hot_maker(Y_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
