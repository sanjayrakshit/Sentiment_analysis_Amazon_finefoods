{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-Analysis-Amazon-fine-foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Reviews.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension after eliminating duplicates (396309, 10)\n"
     ]
    }
   ],
   "source": [
    "imp_cols = set(df.columns)-{'Id','ProductId'}\n",
    "df = df.drop_duplicates(subset=imp_cols)\n",
    "print ('Dimension after eliminating duplicates',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "    1. Neglecting 3 star reviews \n",
    "    2. Sorting by time-stamp\n",
    "    3. Extracting Reviews and Summary and concatenating them\n",
    "    4. Defining <3 score as negative and >3 as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366402, 366402)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.Score != 3]\n",
    "df = df.sort_values(by='Time')\n",
    "temp1 = df.Text.tolist(); temp2 = df.Summary.tolist()\n",
    "X = [str(temp1[i])+' '+str(temp2[i]) for i in range(len(temp1))]\n",
    "Y = []\n",
    "for i in df.Score.tolist():\n",
    "    if(i>3):\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "del df, temp1, temp2\n",
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "* Removing HTML tags\n",
    "* Make in lower case\n",
    "* Tokenizing and removing stopwords with punctuation marks\n",
    "* Also removing non alpha numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'movie', 'movie', 'collection', 'filled', 'comedy', 'action', 'whatever', 'else', 'want', 'call', 'great']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "X = [re.sub('<[^>]*>', '',i.lower()) for i in X] #Removes HTML tags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "stop_word = stopwords.words('english')+\\\n",
    "['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']\n",
    "'''including punctuation marks and other chatecters in the stopwords'''\n",
    "for j in range(len(X)):\n",
    "    X[j] = [i for i in wordpunct_tokenize(X[j]) if (i not in stop_word) and (i.isalnum())]\n",
    "print (X[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming to normalize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for j in range(len(X)):\n",
    "    X[j] = [ps.stem(i) for i in X[j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Storing the cleaned data to avoid running above operations in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean-data-XY.pkl','wb') as fp:\n",
    "    tupXY = (X,Y)\n",
    "    pickle.dump(tupXY,fp)\n",
    "fp.close()\n",
    "del tupXY, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366402, 366402)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('clean-data-XY.pkl','rb') as fp:\n",
    "    X,Y = pickle.load(fp)\n",
    "fp.close()\n",
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'witti littl book make son laugh loud recit car drive along alway sing refrain learn whale india droop rose love new word book introduc silli classic book will bet son still abl recit memori colleg everi book educ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [' '.join(i) for i in X]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count based Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256481, 27224), (109921, 27224))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = CountVectorizer(min_df=3,binary=False) #keeping min_df=3 will eliminate unnecessary strings\n",
    "l = int(0.7*len(X))\n",
    "model.fit(X[:l])\n",
    "BOW_tr = model.transform(X[:l])\n",
    "BOW_ts = model.transform(X[l:])\n",
    "BOW_tr.shape, BOW_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurence based Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256481, 27224), (109921, 27224))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CountVectorizer(min_df=3,binary=True)\n",
    "model.fit(X[:l])\n",
    "BBOW_tr = model.transform(X[:l])\n",
    "BBOW_ts = model.transform(X[l:])\n",
    "BBOW_tr.shape, BBOW_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Baye's\n",
    "    Doing Grid-search on hyperparameter alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(MultinomialNB(),{'alpha':[0.5,0.25,0.125,1,2,3,4]})\n",
    "clf.fit(BOW_tr,Y[:l])\n",
    "hX = clf.predict(BOW_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.7340398 ,  0.94389242]),\n",
       " array([ 0.73643813,  0.94323636]),\n",
       " array([ 0.73523701,  0.94356427]),\n",
       " array([19282, 90639]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve\n",
    "precision_recall_fscore_support(Y[l:],hX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see high precision and recall for positive class, but low for negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoullis Naive Baye's\n",
    "    Doing grid search on hyperparameter alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = GridSearchCV(BernoulliNB(),{'alpha':[0.5,0.25,0.125,1,2,3,4]})\n",
    "clf.fit(BBOW_tr,Y[:l])\n",
    "hX = clf.predict(BBOW_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.69271864,  0.9411627 ]),\n",
       " array([ 0.72627321,  0.93146438]),\n",
       " array([ 0.70909919,  0.93628843]),\n",
       " array([19282, 90639]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(Y[l:],hX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
