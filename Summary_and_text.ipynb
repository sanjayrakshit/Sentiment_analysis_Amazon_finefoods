{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396309, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading the deduped dataframe\n",
    "data = pd.read_pickle('deduped_reviews')\n",
    "#data = data.sort_values(by='Time')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for pre-processing\n",
    "\n",
    "def print_df(df,nn):\n",
    "    '''Prints random nn samples'''\n",
    "    print (df.sample(n=nn))\n",
    "    \n",
    "# Appending summary and text\n",
    "def append_summary_text():\n",
    "    '''This function will append the summary and the text'''\n",
    "    summary = data.Summary.tolist()\n",
    "    text = data.Text.tolist()\n",
    "    for i in range(len(summary)):\n",
    "        if(summary[i][-1]!='.' or summary[i][-1]!='!'):\n",
    "            summary[i] = summary[i] + '.'\n",
    "        text[i] = summary[i]+' '+text[i]\n",
    "    data['Text'] = text\n",
    "\n",
    "def rearrange_score():\n",
    "    '''This function will make every score greater than 3 as positive\n",
    "        and something less than 3 as negative'''\n",
    "    score = data.Score.tolist()\n",
    "    for i in range(len(score)):\n",
    "        if(score[i]>3):\n",
    "            score[i]=1\n",
    "        else:\n",
    "            score[i]=0\n",
    "    data['Score'] = score\n",
    "    \n",
    "def drop_cols(df,cols):\n",
    "    df = df.drop(labels=cols,axis=1)\n",
    "    return df\n",
    "\n",
    "def remove_htmltags(df,cn):\n",
    "    col = df[cn].tolist()\n",
    "    from bs4 import BeautifulSoup\n",
    "    for i in range(len(col)):\n",
    "        soup = BeautifulSoup(col[i], \"lxml\")        \n",
    "        col[i] = soup.get_text()\n",
    "    df[cn] = col\n",
    "    return df\n",
    "\n",
    "def remove_punctuation(df,cn):\n",
    "    col = df[cn].tolist()\n",
    "    import re\n",
    "    for i in range(len(col)):\n",
    "        col[i] = re.sub('[^A-Za-z0-9\\s\\']+', '', col[i])\n",
    "    df[cn] = col\n",
    "    return df\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word = stopwords.words('english')\n",
    "#print (type(stop_word))\n",
    "\n",
    "def notin(word):\n",
    "    return (not(word in set(stop_word)))\n",
    "\n",
    "def remove_stopwords(df,cn):\n",
    "    col = df[cn].tolist()\n",
    "    for i in range(len(col)):\n",
    "        col[i] = ' '.join(j for j in col[i].split() if notin(j))\n",
    "    df[cn] = col\n",
    "    return df\n",
    "\n",
    "def stemming(df,cn):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    col = df[cn].tolist()\n",
    "    for i in range(len(col)):\n",
    "        col[i] = ' '.join(stemmer.stem(j) for j in col[i].split())\n",
    "    df[cn] = col\n",
    "    return df\n",
    "\n",
    "def make_lower(df,cn):\n",
    "    col = df[cn].tolist()\n",
    "    for i in range(len(col)):\n",
    "        col[i] = col[i].lower()\n",
    "    df[cn]=col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id   ProductId          UserId                ProfileName  \\\n",
      "375938  375939  B0000DBN1L  A3D6TFYRMIV3ZL              Themis-Athena   \n",
      "29857    29858  B0045CTYNI  A3B6D3UQIMVOUK  ViVeriVeniversumVivusVici   \n",
      "290881  290882  B005HG9ESG  A2L0WJMOT484GM                   reviewer   \n",
      "428009  428010  B003KRHDMI  A1NZ4QSS7JJ1UY            Packard 1 \"Ren\"   \n",
      "284048  284049  B0051COPH6  A2VH0UT5EQFB6P                 Loveguitar   \n",
      "\n",
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "375938                    11                      12      5  1163116800   \n",
      "29857                      5                       5      5  1318636800   \n",
      "290881                     0                       0      4  1345680000   \n",
      "428009                     1                       1      5  1293753600   \n",
      "284048                     0                       0      4  1342224000   \n",
      "\n",
      "                                  Summary  \\\n",
      "375938  India's Original Spiced Milk Tea.   \n",
      "29857          Addiction with less guilt!   \n",
      "290881                        as expected   \n",
      "428009                     Great Carousel   \n",
      "284048                   Tastes Very Good   \n",
      "\n",
      "                                                     Text  \n",
      "375938  In many languages around the world, \"chai\" sim...  \n",
      "29857   It really is very simple. These things are SO ...  \n",
      "290881  Good big sturdy bottle with nutrient water as ...  \n",
      "428009  This carousel is sturdy and is easy to use. It...  \n",
      "284048  I used to make my baby food from scratch using...  \n"
     ]
    }
   ],
   "source": [
    "# How the data looks\n",
    "print_df(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discarding the three star reviews\n",
    "data = data.loc[data.Score!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366402, 10)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)\n",
    "# Previously there were 396309 reviews. Now 366402. (396309-366402) = ~30k 3 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               11\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                    1\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (data.isna().sum())\n",
    "# Checking where the null values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('Unavailable')\n",
    "# We are filling the null values with unavailable as filling with string type is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                        0\n",
      "ProductId                 0\n",
      "UserId                    0\n",
      "ProfileName               0\n",
      "HelpfulnessNumerator      0\n",
      "HelpfulnessDenominator    0\n",
      "Score                     0\n",
      "Time                      0\n",
      "Summary                   0\n",
      "Text                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (data.isna().sum())\n",
    "# No null values anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_summary_text() # Calling this function would append the summary infront of the text\n",
    "data = make_lower(data,'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id   ProductId          UserId            ProfileName  \\\n",
      "70095    70096  B007I7Z3Z0   AKGQ6RM68SQY1  Catherine Diane \"CDI\"   \n",
      "381117  381118  B001EQ4HVC   APPPI44BNFTMF         Ellen A. Paige   \n",
      "20100    20101  B004U49R24   AVU1ILDDYW301               G. Hearn   \n",
      "138719  138720  B0000GHNTK  A1X1CEGHTHMBL1                  jjceo   \n",
      "384195  384196  B000EVWQZW  A2AJ8FFJED971Z        atticusthebaker   \n",
      "\n",
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "70095                      1                       2      1  1343606400   \n",
      "381117                     8                       8      5  1196553600   \n",
      "20100                      1                       1      2  1336521600   \n",
      "138719                     0                       0      5  1340323200   \n",
      "384195                     2                       4      1  1288310400   \n",
      "\n",
      "                                 Summary  \\\n",
      "70095                          SULFITES!   \n",
      "381117  S'Malts, as in \"some more malts\"   \n",
      "20100           Too sweet and just yucky   \n",
      "138719                             Ouch!   \n",
      "384195                        Play dough   \n",
      "\n",
      "                                                     Text  \n",
      "70095   sulfites!. if you have to avoid them avoid thi...  \n",
      "381117  s'malts, as in \"some more malts\". nostalgia mo...  \n",
      "20100   too sweet and just yucky. i like \"regular\" ore...  \n",
      "138719  ouch!. i love to spice up my food whether it i...  \n",
      "384195  play dough. my first loaf of this excuse for b...  \n"
     ]
    }
   ],
   "source": [
    "# Let's check how the dataframe looks now\n",
    "print_df(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange_score() # Calling this function would rearrange the score 1 for >3 and 0 for <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id   ProductId          UserId                       ProfileName  \\\n",
      "431965  431966  B001P22GHC  A2JOLGWW0J7K6G                             angel   \n",
      "75687    75688  B004MO6NI8   AOYUWL023B8TZ  Donna \"a very truthful reviewer\"   \n",
      "121057  121058  B001JH93BU  A35CSDXCHC44SY              Frequent Contributor   \n",
      "\n",
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "431965                     2                       2      1  1275609600   \n",
      "75687                      0                       0      1  1330387200   \n",
      "121057                    52                      55      1  1264204800   \n",
      "\n",
      "                          Summary  \\\n",
      "431965  Pre-portioned yummy-ness!   \n",
      "75687                 Great Taste   \n",
      "121057                Fun once...   \n",
      "\n",
      "                                                     Text  \n",
      "431965  pre-portioned yummy-ness!. i initially began t...  \n",
      "75687   great taste. i enjoyed the taste of no fear, b...  \n",
      "121057  fun once.... a lot of the reviews/testimonials...  \n"
     ]
    }
   ],
   "source": [
    "# Let's check how the dataframe looks\n",
    "print_df(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going the drop the columns that we don't need\n",
    "cols_to_drop = set(data.columns) - {'Score','Text','HelpfulnessNumerator','HelpfulnessDenominator'}\n",
    "\n",
    "data = drop_cols(data,list(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
      "443525                     1                       3      1   \n",
      "48714                      2                       2      0   \n",
      "325477                     0                       0      1   \n",
      "\n",
      "                                                     Text  \n",
      "443525  these candy pebbles rock!. (sing to \"tiny bubb...  \n",
      "48714   awful taste, not even sure it's real honey, be...  \n",
      "325477  it is good.. i don't write a lot of reviews......  \n"
     ]
    }
   ],
   "source": [
    "print_df(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_htmltags(data,'Text') # This will remove the html tags that we have in the 'Text' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delicious!!!. i am a runner and a single mom so this soup is perfect for my taste and nutritional needs.  i eat them for lunch, a healthy snack or sometimes breakfast.  i have tried many cup-o-soups, but this one is my favorite.  it isn't overly salty and has nutritional yeast in it, which a flavor i love.  i subscribe to this product.  i appreciate having it delivered to me.  i can't always find it in stores, and even if they carry it isn't always on the shelf.  i highly recommend this soup.\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "absolutely delicious!. these individually wrapped pieces are delicious (although i wish there were a few more). mildly sweet with no sugar-free aftertaste, even a traditional \"marzipan-aholic\" should enjoy these.\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "yum?. i quite liked it, the flavor is a bit harsh at first but then it becomes more mellow. it is most certainly caffeinated which is good. it is too expensive to buy more than once though. all in all i liked it.\n"
     ]
    }
   ],
   "source": [
    "print (data.Text.iloc[290620]);print('-'*125); print(data.Text.iloc[293]);print('-'*125); print(data.Text.iloc[156971])\n",
    "# We have picked three random reviews and they seem free from tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_punctuation(data,'Text') # This will remove the puntuation except apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delicious i am a runner and a single mom so this soup is perfect for my taste and nutritional needs  i eat them for lunch a healthy snack or sometimes breakfast  i have tried many cuposoups but this one is my favorite  it isn't overly salty and has nutritional yeast in it which a flavor i love  i subscribe to this product  i appreciate having it delivered to me  i can't always find it in stores and even if they carry it isn't always on the shelf  i highly recommend this soup\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "absolutely delicious these individually wrapped pieces are delicious although i wish there were a few more mildly sweet with no sugarfree aftertaste even a traditional marzipanaholic should enjoy these\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "yum i quite liked it the flavor is a bit harsh at first but then it becomes more mellow it is most certainly caffeinated which is good it is too expensive to buy more than once though all in all i liked it\n"
     ]
    }
   ],
   "source": [
    "print (data.Text.iloc[290620]);print('-'*125); print(data.Text.iloc[293]);print('-'*125); print(data.Text.iloc[156971])\n",
    "# We have picked three random reviews and they seem free from punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_stopwords(data,'Text') # This will remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delicious runner single mom soup perfect taste nutritional needs eat lunch healthy snack sometimes breakfast tried many cuposoups one favorite overly salty nutritional yeast flavor love subscribe product appreciate delivered can't always find stores even carry always shelf highly recommend soup\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "absolutely delicious individually wrapped pieces delicious although wish mildly sweet sugarfree aftertaste even traditional marzipanaholic enjoy\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "yum quite liked flavor bit harsh first becomes mellow certainly caffeinated good expensive buy though liked\n"
     ]
    }
   ],
   "source": [
    "print (data.Text.iloc[290620]);print('-'*125); print(data.Text.iloc[293]);print('-'*125); print(data.Text.iloc[156971])\n",
    "# We have picked three random reviews and they seem free from punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the processed dataframe\n",
    "data.to_pickle('processed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('processed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data = stemming(data,'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the stemmed data too for later usage\n",
    "data.to_pickle('processed_stemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delici runner singl mom soup perfect tast nutrit need eat lunch healthi snack sometim breakfast tri mani cuposoup one favorit overli salti nutrit yeast flavor love subscrib product appreci deliv can't alway find store even carri alway shelf highli recommend soup\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "absolut delici individu wrap piec delici although wish mildli sweet sugarfre aftertast even tradit marzipanahol enjoy\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "yum quit like flavor bit harsh first becom mellow certainli caffein good expens buy though like\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('processed_stemmed_reviews')\n",
    "print (data.Text.iloc[290620]);print('-'*125); print(data.Text.iloc[293]);print('-'*125); print(data.Text.iloc[156971])\n",
    "# We have picked three random reviews and they seem free from punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjay\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Utility functions and algorithm\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def metric(observed,predicted):\n",
    "    '''Prints different metric btaking in observed and predicted value'''\n",
    "    pre_rec = precision_recall_fscore_support(observed,predicted)\n",
    "    roc_auc = roc_auc_score(observed,predicted)\n",
    "    print ('---Precision:---\\n{}\\n---Recall:---\\n{}\\n---fscore:---\\n{}\\n---AUC:---\\n{}'.format(pre_rec[0],pre_rec[1],pre_rec[2],roc_auc))\n",
    "    \n",
    "    \n",
    "def lr_classifier(X_train,X_test,y_train,param):\n",
    "    '''Logistic regression with hyperparameter tuning'''\n",
    "    lr = LogisticRegression(class_weight= 'balanced',n_jobs=-1,penalty='l1')\n",
    "    clf = GridSearchCV(lr,param)\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    lr_parameters = lr.get_params()\n",
    "    lr_parameters['C'] = clf.best_params_['C']\n",
    "\n",
    "    lr.set_params(**lr_parameters)\n",
    "    print ('\\n---Parameters for LR---\\n{}'.format(lr.get_params))\n",
    "\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    return (y_pred) \n",
    "\n",
    "def nb_classifier(X_train,X_test,y_train,param):\n",
    "    '''Naive Bayes with hyper parameter tuning'''\n",
    "    nb = MultinomialNB(class_prior=[1,1])\n",
    "    clf = GridSearchCV(nb,param)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    nb_parameters = nb.get_params()\n",
    "    nb_parameters['alpha'] = clf.best_params_['alpha']\n",
    "\n",
    "    nb.set_params(**nb_parameters)\n",
    "    print ('\\n---Parameters for NB---\\n{}'.format(nb.get_params))\n",
    "\n",
    "    nb.fit(X_train,y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    \n",
    "    return (y_pred) \n",
    "\n",
    "def rf_classifier(X_train,X_test,y_train,param):\n",
    "    '''Random forest with hyperparameter tuning as number of trees'''\n",
    "    rf = RandomForestClassifier(n_jobs=-1,class_weight='balanced',verbose=0)\n",
    "    clf = GridSearchCV(rf,param)\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    rf_parameters = rf.get_params()\n",
    "    rf_parameters['n_estimators'] = clf.best_params_['n_estimators']\n",
    "\n",
    "    rf.set_params(**rf_parameters)\n",
    "    print ('\\n---Parameters for RF---\\n{}'.format(rf.get_params))\n",
    "\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    return (y_pred)\n",
    "\n",
    "def xgb_classifier(X_train,X_test,y_train,param):\n",
    "    \n",
    "    xg = XGBClassifier(silent=True,nthread=4) # Change this n_thread according to the number of cores\n",
    "    clf = GridSearchCV(xg,param)\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    xg_parameters = xg.get_params()\n",
    "    xg_parameters['n_estimators'] = clf.best_params_['n_estimators']\n",
    "    xg_parameters['max_depth'] = clf.best_params_['max_depth']\n",
    "\n",
    "    xg.set_params(**xg_parameters)\n",
    "    print ('\\n---Parameters for xgboost---\\n{}'.format(xg.get_params))\n",
    "\n",
    "    xg.fit(X_train,y_train)\n",
    "    y_pred = xg.predict(X_test)\n",
    "\n",
    "    return (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW approach on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set -- 293121\n",
      "Size of test set -- 73281\n"
     ]
    }
   ],
   "source": [
    "l = int(0.8*data.shape[0])\n",
    "print ('Size of training set -- {}\\nSize of test set -- {}'.format(l,data.shape[0]-l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366402, 278683)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_vectorizer = CountVectorizer()\n",
    "text_features = text_vectorizer.fit_transform(data.Text)\n",
    "text_features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for LR---\n",
      "<bound method BaseEstimator.get_params of LogisticRegression(C=2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.69488575  0.97097284]\n",
      "---Recall:---\n",
      "[ 0.84724925  0.93213392]\n",
      "---fscore:---\n",
      "[ 0.76354071  0.95115707]\n",
      "---AUC:---\n",
      "0.8896915865782747\n"
     ]
    }
   ],
   "source": [
    "# Performing Logistic regression on BOW approach\n",
    "\n",
    "C = np.arange(1,11,1)\n",
    "parameter = {'C':list(C)}\n",
    "\n",
    "y_pred = lr_classifier(text_features[:l],text_features[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for NB---\n",
      "<bound method BaseEstimator.get_params of MultinomialNB(alpha=1, class_prior=[1, 1], fit_prior=True)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.68177071  0.95364434]\n",
      "---Recall:---\n",
      "[ 0.75057492  0.93608713]\n",
      "---fscore:---\n",
      "[ 0.71452027  0.94478418]\n",
      "---AUC:---\n",
      "0.8433310239409232\n"
     ]
    }
   ],
   "source": [
    "# Performing Naive Bayes on BOW approach\n",
    "\n",
    "alpha = [0.125,0.25,0.5,1,2,4,8]\n",
    "parameter = {'alpha':alpha}\n",
    "\n",
    "y_pred = nb_classifier(text_features[:l],text_features[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note]:\n",
    "We cannot apply Random forest, xgboost or any of it's variations since the dimensionality is huge when we are applying BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf approach on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366402, 278683)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_text_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tfidf_text_features = tfidf_text_vectorizer.fit_transform(data['Text'])\n",
    "tfidf_text_features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for LR---\n",
      "<bound method BaseEstimator.get_params of LogisticRegression(C=6, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.68664831  0.97325688]\n",
      "---Recall:---\n",
      "[ 0.86016275  0.92839048]\n",
      "---fscore:---\n",
      "[ 0.76367349  0.9502944 ]\n",
      "---AUC:---\n",
      "0.8942766127385838\n"
     ]
    }
   ],
   "source": [
    "# Performing Logistic regression on Tfidf approach\n",
    "\n",
    "C = np.arange(1,11,1)\n",
    "parameter = {'C':list(C)}\n",
    "\n",
    "y_pred = lr_classifier(tfidf_text_features[:l],tfidf_text_features[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for NB---\n",
      "<bound method BaseEstimator.get_params of MultinomialNB(alpha=0.25, class_prior=[1, 1], fit_prior=True)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.62240332  0.94213774]\n",
      "---Recall:---\n",
      "[ 0.68901468  0.92374344]\n",
      "---fscore:---\n",
      "[ 0.65401729  0.93284993]\n",
      "---AUC:---\n",
      "0.80637906370348\n"
     ]
    }
   ],
   "source": [
    "# Performing Naive Bayes on tfidf approach\n",
    "\n",
    "alpha = [0.125,0.25,0.5,1,2,4,8]\n",
    "parameter = {'alpha':alpha}\n",
    "\n",
    "y_pred = nb_classifier(tfidf_text_features[:l],tfidf_text_features[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note]: Due to the dimensionality of the data, we cannot apply any form of decision tree based methods. The same reason was for BOW approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading unstemmed data \n",
    "data = pd.read_pickle('processed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and loading google's w2v model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for W2v model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def center_scale(X):\n",
    "    '''This function standardizes the features'''\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def get_avg_sen_vector(sentence,weight=1):\n",
    "    '''This function produces the mean weighted sentence vector'''\n",
    "    _l = 0\n",
    "    sen_vec = np.zeros(shape=(300,))\n",
    "    for i in sentence.split():\n",
    "        try:\n",
    "            sen_vec += model[i]*weight\n",
    "            _l += 1\n",
    "        except:\n",
    "            a=None\n",
    "    sen_vec = sen_vec/_l\n",
    "    return (sen_vec)\n",
    "    \n",
    "def get_vector(df,way='avg'):\n",
    "    '''This function gets the vector of the whole dataset be it mean weighted or idf weighted'''\n",
    "    w2v_vectors = []\n",
    "    text = df.Text.tolist()\n",
    "    for i in range(len(text)):\n",
    "        if(way=='avg'):\n",
    "            w2v_vectors.append(get_avg_sen_vector(text[i]))\n",
    "    return w2v_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 300 dim mean weighted vector\n",
    "avg_w2v = get_vector(data,'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 366402 x 300\n"
     ]
    }
   ],
   "source": [
    "print ('Dimensions: {} x {}'.format(len(avg_w2v),len(avg_w2v[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172301 0\n",
      "172301 1\n",
      "172301 2\n",
      "172301 3\n",
      "172301 4\n",
      "172301 5\n",
      "172301 6\n",
      "172301 7\n",
      "172301 8\n",
      "172301 9\n",
      "172301 10\n",
      "172301 11\n",
      "172301 12\n",
      "172301 13\n",
      "172301 14\n",
      "172301 15\n",
      "172301 16\n",
      "172301 17\n",
      "172301 18\n",
      "172301 19\n",
      "172301 20\n",
      "172301 21\n",
      "172301 22\n",
      "172301 23\n",
      "172301 24\n",
      "172301 25\n",
      "172301 26\n",
      "172301 27\n",
      "172301 28\n",
      "172301 29\n",
      "172301 30\n",
      "172301 31\n",
      "172301 32\n",
      "172301 33\n",
      "172301 34\n",
      "172301 35\n",
      "172301 36\n",
      "172301 37\n",
      "172301 38\n",
      "172301 39\n",
      "172301 40\n",
      "172301 41\n",
      "172301 42\n",
      "172301 43\n",
      "172301 44\n",
      "172301 45\n",
      "172301 46\n",
      "172301 47\n",
      "172301 48\n",
      "172301 49\n",
      "172301 50\n",
      "172301 51\n",
      "172301 52\n",
      "172301 53\n",
      "172301 54\n",
      "172301 55\n",
      "172301 56\n",
      "172301 57\n",
      "172301 58\n",
      "172301 59\n",
      "172301 60\n",
      "172301 61\n",
      "172301 62\n",
      "172301 63\n",
      "172301 64\n",
      "172301 65\n",
      "172301 66\n",
      "172301 67\n",
      "172301 68\n",
      "172301 69\n",
      "172301 70\n",
      "172301 71\n",
      "172301 72\n",
      "172301 73\n",
      "172301 74\n",
      "172301 75\n",
      "172301 76\n",
      "172301 77\n",
      "172301 78\n",
      "172301 79\n",
      "172301 80\n",
      "172301 81\n",
      "172301 82\n",
      "172301 83\n",
      "172301 84\n",
      "172301 85\n",
      "172301 86\n",
      "172301 87\n",
      "172301 88\n",
      "172301 89\n",
      "172301 90\n",
      "172301 91\n",
      "172301 92\n",
      "172301 93\n",
      "172301 94\n",
      "172301 95\n",
      "172301 96\n",
      "172301 97\n",
      "172301 98\n",
      "172301 99\n",
      "172301 100\n",
      "172301 101\n",
      "172301 102\n",
      "172301 103\n",
      "172301 104\n",
      "172301 105\n",
      "172301 106\n",
      "172301 107\n",
      "172301 108\n",
      "172301 109\n",
      "172301 110\n",
      "172301 111\n",
      "172301 112\n",
      "172301 113\n",
      "172301 114\n",
      "172301 115\n",
      "172301 116\n",
      "172301 117\n",
      "172301 118\n",
      "172301 119\n",
      "172301 120\n",
      "172301 121\n",
      "172301 122\n",
      "172301 123\n",
      "172301 124\n",
      "172301 125\n",
      "172301 126\n",
      "172301 127\n",
      "172301 128\n",
      "172301 129\n",
      "172301 130\n",
      "172301 131\n",
      "172301 132\n",
      "172301 133\n",
      "172301 134\n",
      "172301 135\n",
      "172301 136\n",
      "172301 137\n",
      "172301 138\n",
      "172301 139\n",
      "172301 140\n",
      "172301 141\n",
      "172301 142\n",
      "172301 143\n",
      "172301 144\n",
      "172301 145\n",
      "172301 146\n",
      "172301 147\n",
      "172301 148\n",
      "172301 149\n",
      "172301 150\n",
      "172301 151\n",
      "172301 152\n",
      "172301 153\n",
      "172301 154\n",
      "172301 155\n",
      "172301 156\n",
      "172301 157\n",
      "172301 158\n",
      "172301 159\n",
      "172301 160\n",
      "172301 161\n",
      "172301 162\n",
      "172301 163\n",
      "172301 164\n",
      "172301 165\n",
      "172301 166\n",
      "172301 167\n",
      "172301 168\n",
      "172301 169\n",
      "172301 170\n",
      "172301 171\n",
      "172301 172\n",
      "172301 173\n",
      "172301 174\n",
      "172301 175\n",
      "172301 176\n",
      "172301 177\n",
      "172301 178\n",
      "172301 179\n",
      "172301 180\n",
      "172301 181\n",
      "172301 182\n",
      "172301 183\n",
      "172301 184\n",
      "172301 185\n",
      "172301 186\n",
      "172301 187\n",
      "172301 188\n",
      "172301 189\n",
      "172301 190\n",
      "172301 191\n",
      "172301 192\n",
      "172301 193\n",
      "172301 194\n",
      "172301 195\n",
      "172301 196\n",
      "172301 197\n",
      "172301 198\n",
      "172301 199\n",
      "172301 200\n",
      "172301 201\n",
      "172301 202\n",
      "172301 203\n",
      "172301 204\n",
      "172301 205\n",
      "172301 206\n",
      "172301 207\n",
      "172301 208\n",
      "172301 209\n",
      "172301 210\n",
      "172301 211\n",
      "172301 212\n",
      "172301 213\n",
      "172301 214\n",
      "172301 215\n",
      "172301 216\n",
      "172301 217\n",
      "172301 218\n",
      "172301 219\n",
      "172301 220\n",
      "172301 221\n",
      "172301 222\n",
      "172301 223\n",
      "172301 224\n",
      "172301 225\n",
      "172301 226\n",
      "172301 227\n",
      "172301 228\n",
      "172301 229\n",
      "172301 230\n",
      "172301 231\n",
      "172301 232\n",
      "172301 233\n",
      "172301 234\n",
      "172301 235\n",
      "172301 236\n",
      "172301 237\n",
      "172301 238\n",
      "172301 239\n",
      "172301 240\n",
      "172301 241\n",
      "172301 242\n",
      "172301 243\n",
      "172301 244\n",
      "172301 245\n",
      "172301 246\n",
      "172301 247\n",
      "172301 248\n",
      "172301 249\n",
      "172301 250\n",
      "172301 251\n",
      "172301 252\n",
      "172301 253\n",
      "172301 254\n",
      "172301 255\n",
      "172301 256\n",
      "172301 257\n",
      "172301 258\n",
      "172301 259\n",
      "172301 260\n",
      "172301 261\n",
      "172301 262\n",
      "172301 263\n",
      "172301 264\n",
      "172301 265\n",
      "172301 266\n",
      "172301 267\n",
      "172301 268\n",
      "172301 269\n",
      "172301 270\n",
      "172301 271\n",
      "172301 272\n",
      "172301 273\n",
      "172301 274\n",
      "172301 275\n",
      "172301 276\n",
      "172301 277\n",
      "172301 278\n",
      "172301 279\n",
      "172301 280\n",
      "172301 281\n",
      "172301 282\n",
      "172301 283\n",
      "172301 284\n",
      "172301 285\n",
      "172301 286\n",
      "172301 287\n",
      "172301 288\n",
      "172301 289\n",
      "172301 290\n",
      "172301 291\n",
      "172301 292\n",
      "172301 293\n",
      "172301 294\n",
      "172301 295\n",
      "172301 296\n",
      "172301 297\n",
      "172301 298\n",
      "172301 299\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for i in range(len(avg_w2v)):\n",
    "    for j in range(len(avg_w2v[i])):\n",
    "        if(math.isnan(avg_w2v[i][j] or math.isinf(avg_w2v[i][j]))):\n",
    "            print (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n"
     ]
    }
   ],
   "source": [
    "print (avg_w2v[172301])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we notice that there's something wrong with the location 172301. Everything is NaN here. However, one entry cannot harm our model that much. Hence let's change the nan there and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w2v[172301] = list(np.zeros(shape=(300,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering and scaling the data, so that the optimisation algorithm converges faster\n",
    "avg_w2v = center_scale(avg_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for LR---\n",
      "<bound method BaseEstimator.get_params of LogisticRegression(C=8, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.52353286  0.97278063]\n",
      "---Recall:---\n",
      "[ 0.86874226  0.85576442]\n",
      "---fscore:---\n",
      "[ 0.65334087  0.91052835]\n",
      "---AUC:---\n",
      "0.8622533409420308\n"
     ]
    }
   ],
   "source": [
    "# Performing Logistic regression on mean weighted w2v approach\n",
    "\n",
    "C = [0.125,0.25,0.5,1,2,4,8]\n",
    "parameter = {'C':C}\n",
    "\n",
    "y_pred = lr_classifier(avg_w2v[:l],avg_w2v[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest on mean weighted w2v \n",
    "\n",
    "parameter = {'n_estimators':[50,100,150,200,250]}\n",
    "\n",
    "y_pred = rf_classifier(avg_w2v[:l],avg_w2v[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost on mean weighted w2v\n",
    "\n",
    "parameter = {'n_estimators':[50,100,150,200,250], 'max_depth':[1,2,3,4,5]}\n",
    "\n",
    "y_pred = xgb_classifier(avg_w2v[:l],avg_w2v[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would try to do tfidf weighted w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for tfidf weighted w2v\n",
    "\n",
    "def get_tfidfweighted_w2v(df):\n",
    "    '''This function will take in the dataframe and return the tfidf weighted word 2 vec'''\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=0)\n",
    "    tfidf_features = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "    tfidf_vec = []\n",
    "    \n",
    "    check = 0; words_not_in = [] # Debugging purposes\n",
    "    \n",
    "    text = df['Text'].tolist()\n",
    "    \n",
    "    for loc in range(len(text)):\n",
    "        sentence = text[loc]\n",
    "        sen_vec = np.zeros(shape=(300,)); N = 0\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                if(word in tfidf_vectorizer.vocabulary_.keys()):\n",
    "                    x = loc\n",
    "                    y = tfidf_vectorizer.vocabulary_[word]\n",
    "                    tfidf_weight = tfidf_features[x,y]\n",
    "                    #print (tfidf_weight)\n",
    "                    sen_vec += tfidf_weight * model[word]\n",
    "                    N += tfidf_weight\n",
    "            except:\n",
    "                #print ('Entered')\n",
    "                check = check + 1\n",
    "                words_not_in.append(word)\n",
    "        sen_vec = sen_vec / N\n",
    "        tfidf_vec.append(sen_vec)\n",
    "        \n",
    "    return tfidf_vec,words_not_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_w2v,word_not_in =  get_tfidfweighted_w2v(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172301 0\n",
      "172301 1\n",
      "172301 2\n",
      "172301 3\n",
      "172301 4\n",
      "172301 5\n",
      "172301 6\n",
      "172301 7\n",
      "172301 8\n",
      "172301 9\n",
      "172301 10\n",
      "172301 11\n",
      "172301 12\n",
      "172301 13\n",
      "172301 14\n",
      "172301 15\n",
      "172301 16\n",
      "172301 17\n",
      "172301 18\n",
      "172301 19\n",
      "172301 20\n",
      "172301 21\n",
      "172301 22\n",
      "172301 23\n",
      "172301 24\n",
      "172301 25\n",
      "172301 26\n",
      "172301 27\n",
      "172301 28\n",
      "172301 29\n",
      "172301 30\n",
      "172301 31\n",
      "172301 32\n",
      "172301 33\n",
      "172301 34\n",
      "172301 35\n",
      "172301 36\n",
      "172301 37\n",
      "172301 38\n",
      "172301 39\n",
      "172301 40\n",
      "172301 41\n",
      "172301 42\n",
      "172301 43\n",
      "172301 44\n",
      "172301 45\n",
      "172301 46\n",
      "172301 47\n",
      "172301 48\n",
      "172301 49\n",
      "172301 50\n",
      "172301 51\n",
      "172301 52\n",
      "172301 53\n",
      "172301 54\n",
      "172301 55\n",
      "172301 56\n",
      "172301 57\n",
      "172301 58\n",
      "172301 59\n",
      "172301 60\n",
      "172301 61\n",
      "172301 62\n",
      "172301 63\n",
      "172301 64\n",
      "172301 65\n",
      "172301 66\n",
      "172301 67\n",
      "172301 68\n",
      "172301 69\n",
      "172301 70\n",
      "172301 71\n",
      "172301 72\n",
      "172301 73\n",
      "172301 74\n",
      "172301 75\n",
      "172301 76\n",
      "172301 77\n",
      "172301 78\n",
      "172301 79\n",
      "172301 80\n",
      "172301 81\n",
      "172301 82\n",
      "172301 83\n",
      "172301 84\n",
      "172301 85\n",
      "172301 86\n",
      "172301 87\n",
      "172301 88\n",
      "172301 89\n",
      "172301 90\n",
      "172301 91\n",
      "172301 92\n",
      "172301 93\n",
      "172301 94\n",
      "172301 95\n",
      "172301 96\n",
      "172301 97\n",
      "172301 98\n",
      "172301 99\n",
      "172301 100\n",
      "172301 101\n",
      "172301 102\n",
      "172301 103\n",
      "172301 104\n",
      "172301 105\n",
      "172301 106\n",
      "172301 107\n",
      "172301 108\n",
      "172301 109\n",
      "172301 110\n",
      "172301 111\n",
      "172301 112\n",
      "172301 113\n",
      "172301 114\n",
      "172301 115\n",
      "172301 116\n",
      "172301 117\n",
      "172301 118\n",
      "172301 119\n",
      "172301 120\n",
      "172301 121\n",
      "172301 122\n",
      "172301 123\n",
      "172301 124\n",
      "172301 125\n",
      "172301 126\n",
      "172301 127\n",
      "172301 128\n",
      "172301 129\n",
      "172301 130\n",
      "172301 131\n",
      "172301 132\n",
      "172301 133\n",
      "172301 134\n",
      "172301 135\n",
      "172301 136\n",
      "172301 137\n",
      "172301 138\n",
      "172301 139\n",
      "172301 140\n",
      "172301 141\n",
      "172301 142\n",
      "172301 143\n",
      "172301 144\n",
      "172301 145\n",
      "172301 146\n",
      "172301 147\n",
      "172301 148\n",
      "172301 149\n",
      "172301 150\n",
      "172301 151\n",
      "172301 152\n",
      "172301 153\n",
      "172301 154\n",
      "172301 155\n",
      "172301 156\n",
      "172301 157\n",
      "172301 158\n",
      "172301 159\n",
      "172301 160\n",
      "172301 161\n",
      "172301 162\n",
      "172301 163\n",
      "172301 164\n",
      "172301 165\n",
      "172301 166\n",
      "172301 167\n",
      "172301 168\n",
      "172301 169\n",
      "172301 170\n",
      "172301 171\n",
      "172301 172\n",
      "172301 173\n",
      "172301 174\n",
      "172301 175\n",
      "172301 176\n",
      "172301 177\n",
      "172301 178\n",
      "172301 179\n",
      "172301 180\n",
      "172301 181\n",
      "172301 182\n",
      "172301 183\n",
      "172301 184\n",
      "172301 185\n",
      "172301 186\n",
      "172301 187\n",
      "172301 188\n",
      "172301 189\n",
      "172301 190\n",
      "172301 191\n",
      "172301 192\n",
      "172301 193\n",
      "172301 194\n",
      "172301 195\n",
      "172301 196\n",
      "172301 197\n",
      "172301 198\n",
      "172301 199\n",
      "172301 200\n",
      "172301 201\n",
      "172301 202\n",
      "172301 203\n",
      "172301 204\n",
      "172301 205\n",
      "172301 206\n",
      "172301 207\n",
      "172301 208\n",
      "172301 209\n",
      "172301 210\n",
      "172301 211\n",
      "172301 212\n",
      "172301 213\n",
      "172301 214\n",
      "172301 215\n",
      "172301 216\n",
      "172301 217\n",
      "172301 218\n",
      "172301 219\n",
      "172301 220\n",
      "172301 221\n",
      "172301 222\n",
      "172301 223\n",
      "172301 224\n",
      "172301 225\n",
      "172301 226\n",
      "172301 227\n",
      "172301 228\n",
      "172301 229\n",
      "172301 230\n",
      "172301 231\n",
      "172301 232\n",
      "172301 233\n",
      "172301 234\n",
      "172301 235\n",
      "172301 236\n",
      "172301 237\n",
      "172301 238\n",
      "172301 239\n",
      "172301 240\n",
      "172301 241\n",
      "172301 242\n",
      "172301 243\n",
      "172301 244\n",
      "172301 245\n",
      "172301 246\n",
      "172301 247\n",
      "172301 248\n",
      "172301 249\n",
      "172301 250\n",
      "172301 251\n",
      "172301 252\n",
      "172301 253\n",
      "172301 254\n",
      "172301 255\n",
      "172301 256\n",
      "172301 257\n",
      "172301 258\n",
      "172301 259\n",
      "172301 260\n",
      "172301 261\n",
      "172301 262\n",
      "172301 263\n",
      "172301 264\n",
      "172301 265\n",
      "172301 266\n",
      "172301 267\n",
      "172301 268\n",
      "172301 269\n",
      "172301 270\n",
      "172301 271\n",
      "172301 272\n",
      "172301 273\n",
      "172301 274\n",
      "172301 275\n",
      "172301 276\n",
      "172301 277\n",
      "172301 278\n",
      "172301 279\n",
      "172301 280\n",
      "172301 281\n",
      "172301 282\n",
      "172301 283\n",
      "172301 284\n",
      "172301 285\n",
      "172301 286\n",
      "172301 287\n",
      "172301 288\n",
      "172301 289\n",
      "172301 290\n",
      "172301 291\n",
      "172301 292\n",
      "172301 293\n",
      "172301 294\n",
      "172301 295\n",
      "172301 296\n",
      "172301 297\n",
      "172301 298\n",
      "172301 299\n",
      "257702 0\n",
      "257702 1\n",
      "257702 2\n",
      "257702 3\n",
      "257702 4\n",
      "257702 5\n",
      "257702 6\n",
      "257702 7\n",
      "257702 8\n",
      "257702 9\n",
      "257702 10\n",
      "257702 11\n",
      "257702 12\n",
      "257702 13\n",
      "257702 14\n",
      "257702 15\n",
      "257702 16\n",
      "257702 17\n",
      "257702 18\n",
      "257702 19\n",
      "257702 20\n",
      "257702 21\n",
      "257702 22\n",
      "257702 23\n",
      "257702 24\n",
      "257702 25\n",
      "257702 26\n",
      "257702 27\n",
      "257702 28\n",
      "257702 29\n",
      "257702 30\n",
      "257702 31\n",
      "257702 32\n",
      "257702 33\n",
      "257702 34\n",
      "257702 35\n",
      "257702 36\n",
      "257702 37\n",
      "257702 38\n",
      "257702 39\n",
      "257702 40\n",
      "257702 41\n",
      "257702 42\n",
      "257702 43\n",
      "257702 44\n",
      "257702 45\n",
      "257702 46\n",
      "257702 47\n",
      "257702 48\n",
      "257702 49\n",
      "257702 50\n",
      "257702 51\n",
      "257702 52\n",
      "257702 53\n",
      "257702 54\n",
      "257702 55\n",
      "257702 56\n",
      "257702 57\n",
      "257702 58\n",
      "257702 59\n",
      "257702 60\n",
      "257702 61\n",
      "257702 62\n",
      "257702 63\n",
      "257702 64\n",
      "257702 65\n",
      "257702 66\n",
      "257702 67\n",
      "257702 68\n",
      "257702 69\n",
      "257702 70\n",
      "257702 71\n",
      "257702 72\n",
      "257702 73\n",
      "257702 74\n",
      "257702 75\n",
      "257702 76\n",
      "257702 77\n",
      "257702 78\n",
      "257702 79\n",
      "257702 80\n",
      "257702 81\n",
      "257702 82\n",
      "257702 83\n",
      "257702 84\n",
      "257702 85\n",
      "257702 86\n",
      "257702 87\n",
      "257702 88\n",
      "257702 89\n",
      "257702 90\n",
      "257702 91\n",
      "257702 92\n",
      "257702 93\n",
      "257702 94\n",
      "257702 95\n",
      "257702 96\n",
      "257702 97\n",
      "257702 98\n",
      "257702 99\n",
      "257702 100\n",
      "257702 101\n",
      "257702 102\n",
      "257702 103\n",
      "257702 104\n",
      "257702 105\n",
      "257702 106\n",
      "257702 107\n",
      "257702 108\n",
      "257702 109\n",
      "257702 110\n",
      "257702 111\n",
      "257702 112\n",
      "257702 113\n",
      "257702 114\n",
      "257702 115\n",
      "257702 116\n",
      "257702 117\n",
      "257702 118\n",
      "257702 119\n",
      "257702 120\n",
      "257702 121\n",
      "257702 122\n",
      "257702 123\n",
      "257702 124\n",
      "257702 125\n",
      "257702 126\n",
      "257702 127\n",
      "257702 128\n",
      "257702 129\n",
      "257702 130\n",
      "257702 131\n",
      "257702 132\n",
      "257702 133\n",
      "257702 134\n",
      "257702 135\n",
      "257702 136\n",
      "257702 137\n",
      "257702 138\n",
      "257702 139\n",
      "257702 140\n",
      "257702 141\n",
      "257702 142\n",
      "257702 143\n",
      "257702 144\n",
      "257702 145\n",
      "257702 146\n",
      "257702 147\n",
      "257702 148\n",
      "257702 149\n",
      "257702 150\n",
      "257702 151\n",
      "257702 152\n",
      "257702 153\n",
      "257702 154\n",
      "257702 155\n",
      "257702 156\n",
      "257702 157\n",
      "257702 158\n",
      "257702 159\n",
      "257702 160\n",
      "257702 161\n",
      "257702 162\n",
      "257702 163\n",
      "257702 164\n",
      "257702 165\n",
      "257702 166\n",
      "257702 167\n",
      "257702 168\n",
      "257702 169\n",
      "257702 170\n",
      "257702 171\n",
      "257702 172\n",
      "257702 173\n",
      "257702 174\n",
      "257702 175\n",
      "257702 176\n",
      "257702 177\n",
      "257702 178\n",
      "257702 179\n",
      "257702 180\n",
      "257702 181\n",
      "257702 182\n",
      "257702 183\n",
      "257702 184\n",
      "257702 185\n",
      "257702 186\n",
      "257702 187\n",
      "257702 188\n",
      "257702 189\n",
      "257702 190\n",
      "257702 191\n",
      "257702 192\n",
      "257702 193\n",
      "257702 194\n",
      "257702 195\n",
      "257702 196\n",
      "257702 197\n",
      "257702 198\n",
      "257702 199\n",
      "257702 200\n",
      "257702 201\n",
      "257702 202\n",
      "257702 203\n",
      "257702 204\n",
      "257702 205\n",
      "257702 206\n",
      "257702 207\n",
      "257702 208\n",
      "257702 209\n",
      "257702 210\n",
      "257702 211\n",
      "257702 212\n",
      "257702 213\n",
      "257702 214\n",
      "257702 215\n",
      "257702 216\n",
      "257702 217\n",
      "257702 218\n",
      "257702 219\n",
      "257702 220\n",
      "257702 221\n",
      "257702 222\n",
      "257702 223\n",
      "257702 224\n",
      "257702 225\n",
      "257702 226\n",
      "257702 227\n",
      "257702 228\n",
      "257702 229\n",
      "257702 230\n",
      "257702 231\n",
      "257702 232\n",
      "257702 233\n",
      "257702 234\n",
      "257702 235\n",
      "257702 236\n",
      "257702 237\n",
      "257702 238\n",
      "257702 239\n",
      "257702 240\n",
      "257702 241\n",
      "257702 242\n",
      "257702 243\n",
      "257702 244\n",
      "257702 245\n",
      "257702 246\n",
      "257702 247\n",
      "257702 248\n",
      "257702 249\n",
      "257702 250\n",
      "257702 251\n",
      "257702 252\n",
      "257702 253\n",
      "257702 254\n",
      "257702 255\n",
      "257702 256\n",
      "257702 257\n",
      "257702 258\n",
      "257702 259\n",
      "257702 260\n",
      "257702 261\n",
      "257702 262\n",
      "257702 263\n",
      "257702 264\n",
      "257702 265\n",
      "257702 266\n",
      "257702 267\n",
      "257702 268\n",
      "257702 269\n",
      "257702 270\n",
      "257702 271\n",
      "257702 272\n",
      "257702 273\n",
      "257702 274\n",
      "257702 275\n",
      "257702 276\n",
      "257702 277\n",
      "257702 278\n",
      "257702 279\n",
      "257702 280\n",
      "257702 281\n",
      "257702 282\n",
      "257702 283\n",
      "257702 284\n",
      "257702 285\n",
      "257702 286\n",
      "257702 287\n",
      "257702 288\n",
      "257702 289\n",
      "257702 290\n",
      "257702 291\n",
      "257702 292\n",
      "257702 293\n",
      "257702 294\n",
      "257702 295\n",
      "257702 296\n",
      "257702 297\n",
      "257702 298\n",
      "257702 299\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tfidf_w2v)):\n",
    "    for j in range(len(tfidf_w2v[i])):\n",
    "        if(math.isnan(tfidf_w2v[i][j] or math.isinf(tfidf_w2v[i][j]))):\n",
    "            print (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the null values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_w2v[172301] = list(np.zeros(shape=(300,)))\n",
    "tfidf_w2v[257702] = list(np.zeros(shape=(300,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering and scaling the data, so that the optimisation algorithm converges faster\n",
    "tfidf_w2v = center_scale(tfidf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Parameters for LR---\n",
      "<bound method BaseEstimator.get_params of LogisticRegression(C=2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)>\n",
      "\n",
      "===METRICS===\n",
      "---Precision:---\n",
      "[ 0.45024493  0.9631903 ]\n",
      "---Recall:---\n",
      "[ 0.82920573  0.81529649]\n",
      "---fscore:---\n",
      "[ 0.58360309  0.88309419]\n",
      "---AUC:---\n",
      "0.8222511109951934\n"
     ]
    }
   ],
   "source": [
    "# Performing Logistic regression on mean weighted w2v approach\n",
    "\n",
    "C = [0.125,0.25,0.5,1,2,4,8]\n",
    "parameter = {'C':C}\n",
    "\n",
    "y_pred = lr_classifier(tfidf_w2v[:l],tfidf_w2v[l:],data.Score[:l],parameter)\n",
    "\n",
    "print ('\\n===METRICS===')\n",
    "metric(data.Score[l:],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
